{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "id": "Z54liZ_W3XX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Conv1D, MaxPooling1D,Dropout\n",
        "from keras.layers import Dense, Activation, BatchNormalization, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Set constants\n",
        "MAX_SEQUENCE_LENGTH = 500  # Set your desired sequence length\n",
        "EMBEDDING_DIM = 100  # Adjust as needed\n",
        "QA_EMBED_SIZE = 64\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "# Load IMDb dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Pad sequences\n",
        "train_data = pad_sequences(train_data, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_data, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "with strategy.scope():\n",
        "    # Define a function to create the model\n",
        "    def model1():\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=10000, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
        "        model.add(Conv1D(filters=128, kernel_size=3, padding='valid', activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv1D(filters=64, kernel_size=3, padding='valid', activation='relu'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling1D(4))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(128))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation(\"sigmoid\"))\n",
        "        model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    modelt1 = model1()\n",
        "    modelt1.fit(train_data, train_labels, batch_size=128, epochs=4, validation_split=0.2)"
      ],
      "metadata": {
        "id": "-4O1KlG33cSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Reshape, Dropout, Input, Conv2D, Activation, MaxPooling2D, concatenate, Flatten, Dense, BatchNormalization\n",
        "from keras import optimizers\n",
        "\n",
        "# Set the parameters\n",
        "num_features = 5000\n",
        "sequence_length = 500\n",
        "embedding_dimension = 200\n",
        "\n",
        "# Load the IMDb dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_features)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "X_train = pad_sequences(X_train, maxlen=sequence_length)\n",
        "X_test =  pad_sequences(X_test, maxlen=sequence_length)\n",
        "\n",
        "\n",
        "filter_sizes = [2, 3, 4, 5]\n",
        "\n",
        "\n",
        "\n",
        "# Define the convolutional model\n",
        "def convolution():\n",
        "    inn = Input(shape=(sequence_length, embedding_dimension, 1))\n",
        "\n",
        "    convolutions = []\n",
        "\n",
        "    # Conduct three convolutions and poolings then concatenate them\n",
        "    for f in filter_sizes:\n",
        "        conv = Conv2D(filters=200, kernel_size=(f, embedding_dimension))(inn)\n",
        "        non_linearity = Activation('relu')(conv)\n",
        "        max_pool = MaxPooling2D(pool_size=(sequence_length - f + 1, 1))(non_linearity)\n",
        "        convolutions.append(max_pool)\n",
        "\n",
        "    out = concatenate(convolutions)\n",
        "    model = Model(inputs=inn, outputs=out)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the IMDb CNN model\n",
        "def model2():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=num_features, output_dim=embedding_dimension))\n",
        "    model.add(Reshape((sequence_length, embedding_dimension, 1), input_shape=(sequence_length, embedding_dimension)))\n",
        "#     model.add(Dropout(0.2))\n",
        "    model.add(convolution())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    adam = optimizers.Adam()\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "  modelt2 = model2()\n",
        "  modelt2.fit(X_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P1er1RP_3i0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with strategy.scope():\n",
        "  # Load and preprocess IMDb dataset\n",
        "  (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
        "  x_train = pad_sequences(x_train, maxlen=500)\n",
        "  x_test = pad_sequences(x_test, maxlen=500)\n",
        "\n",
        "  def model3():\n",
        "\n",
        "    # Build the model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Embedding layer\n",
        "    model.add(Embedding(input_dim=10000, output_dim=200))\n",
        "\n",
        "    # Convolutional layer\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(Dropout(0.35))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(MaxPooling1D(4))\n",
        "\n",
        "    # GRU layer\n",
        "    model.add(GRU(250))\n",
        "    model.add(Dropout(0.35))\n",
        "\n",
        "    # Dense layer for classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  modelt3 = model3()\n",
        "\n",
        "    # Train the model\n",
        "  modelt3.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)"
      ],
      "metadata": {
        "id": "x8irWUCB3oQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have three trained models: modelt1, modelt2, and modelt3\n",
        "\n",
        "# Get predictions from each individual model on the test set\n",
        "predictions_model1 = modelt1.predict(X_test)\n",
        "predictions_model2 = modelt2.predict(X_test)\n",
        "predictions_model3 = modelt3.predict(X_test)\n"
      ],
      "metadata": {
        "id": "8-LYcqoe4A_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Weighted Average Ensemble\n",
        "\n",
        "# Define weights for each model (you can adjust these weights based on performance on a validation set)\n",
        "weights = [0.2, 0.5, 0.3]\n",
        "\n",
        "# Calculate weighted average prediction\n",
        "ensemble_predictions = (weights[0] * predictions_model1 +\n",
        "                        weights[1] * predictions_model2 +\n",
        "                        weights[2] * predictions_model3)\n",
        "\n",
        "# Convert to binary predictions\n",
        "ensemble_binary = (ensemble_predictions >= 0.5).astype(int)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy = accuracy_score(y_test, ensemble_binary)\n",
        "print(\"Ensemble Model Accuracy (Weighted Average Ensemble):\", accuracy)"
      ],
      "metadata": {
        "id": "fNqEU-wj-BCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hard Voting ensemble\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions with each model\n",
        "predictions_model1 = modelt1.predict(X_test)\n",
        "predictions_model2 = modelt2.predict(X_test)\n",
        "predictions_model3 = modelt3.predict(X_test)\n",
        "\n",
        "# Calculate the absolute differences from 0.5 for each model's prediction\n",
        "prediction1 = abs(0.53 - predictions_model1)\n",
        "prediction2 = abs(0.53- predictions_model2)\n",
        "prediction3 = abs(0.53- predictions_model3)\n",
        "\n",
        "# Initialize ensemble predictions as an array of zeros\n",
        "ensemble_predictions = np.zeros_like(predictions_model1)\n",
        "\n",
        "# Determine the indices where each model's prediction has the maximum difference from 0.5\n",
        "max_indices_model1 = np.where(prediction1 >= prediction2, prediction1, 0)\n",
        "max_indices_model1 = np.where(prediction1 >= prediction3, max_indices_model1, 0)\n",
        "\n",
        "max_indices_model2 = np.where(prediction2 >= prediction1, prediction2, 0)\n",
        "max_indices_model2 = np.where(prediction2 >= prediction3, max_indices_model2, 0)\n",
        "\n",
        "max_indices_model3 = np.where(prediction3 >= prediction1, prediction3, 0)\n",
        "max_indices_model3 = np.where(prediction3 >= prediction2, max_indices_model3, 0)\n",
        "\n",
        "# Combine the maximum indices across all models\n",
        "ensemble_predictions = np.where(max_indices_model1 != 0, predictions_model1, ensemble_predictions)\n",
        "ensemble_predictions = np.where(max_indices_model2 != 0, predictions_model2, ensemble_predictions)\n",
        "ensemble_predictions = np.where(max_indices_model3 != 0, predictions_model3, ensemble_predictions)\n",
        "\n",
        "# Convert to binary predictions\n",
        "ensemble_binary = (ensemble_predictions >= 0.5).astype(int)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy = accuracy_score(y_test, ensemble_binary)\n",
        "print(\"Ensemble Model Accuracy (Max Voting):\", accuracy)\n"
      ],
      "metadata": {
        "id": "8dQXhU-fvMjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stacking using Logistic Regression\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Generate predictions from each model on the training data\n",
        "train_predictions_model1 = modelt1.predict(X_train)\n",
        "train_predictions_model2 = modelt2.predict(X_train)\n",
        "train_predictions_model3 = modelt3.predict(X_train)\n",
        "\n",
        "# Stack predictions together as features for the meta-model\n",
        "stacked_train_predictions = np.column_stack((train_predictions_model1, train_predictions_model2, train_predictions_model3))\n",
        "\n",
        "# Train the meta-model\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(stacked_train_predictions, y_train)\n",
        "\n",
        "# Generate predictions from each model on the test data\n",
        "test_predictions_model1 = modelt1.predict(X_test)\n",
        "test_predictions_model2 = modelt2.predict(X_test)\n",
        "test_predictions_model3 = modelt3.predict(X_test)\n",
        "\n",
        "# Stack predictions together as features for the meta-model\n",
        "stacked_test_predictions = np.column_stack((test_predictions_model1, test_predictions_model2, test_predictions_model3))\n",
        "\n",
        "# Make final predictions with the meta-model\n",
        "final_predictions = meta_model.predict(stacked_test_predictions)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy = accuracy_score(y_test, final_predictions)\n",
        "print(\"Ensemble Model Accuracy (Stacking):\", accuracy)\n"
      ],
      "metadata": {
        "id": "2m7Z_GfawJDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stacking using SVM\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Generate predictions from each model on the training data\n",
        "train_predictions_model1 = modelt1.predict(X_train)\n",
        "train_predictions_model2 = modelt2.predict(X_train)\n",
        "train_predictions_model3 = modelt3.predict(X_train)\n",
        "\n",
        "# Stack predictions together as features for the meta-model\n",
        "stacked_train_predictions = np.column_stack((train_predictions_model1, train_predictions_model2, train_predictions_model3))\n",
        "\n",
        "# Train the meta-model (SVM)\n",
        "meta_model = SVC()\n",
        "meta_model.fit(stacked_train_predictions, y_train)\n",
        "\n",
        "# Generate predictions from each model on the test data\n",
        "test_predictions_model1 = modelt1.predict(X_test)\n",
        "test_predictions_model2 = modelt2.predict(X_test)\n",
        "test_predictions_model3 = modelt3.predict(X_test)\n",
        "\n",
        "# Stack predictions together as features for the meta-model\n",
        "stacked_test_predictions = np.column_stack((test_predictions_model1, test_predictions_model2, test_predictions_model3))\n",
        "\n",
        "# Make final predictions with the meta-model\n",
        "final_predictions = meta_model.predict(stacked_test_predictions)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy = accuracy_score(y_test, final_predictions)\n",
        "print(\"Ensemble Model Accuracy (Stacking with SVM):\", accuracy)\n"
      ],
      "metadata": {
        "id": "ZwJS1_Yiv4M6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}